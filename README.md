# Wendy AI â€“ Generative AI Customer Service LLM for DC WaterðŸ’§

# App Link: https://wendyai-dcwaterllm.streamlit.app/

# Overview
Wendy AI is a specialized AI-driven customer service platform developed for DC Water. It's designed to handle various water supply chain operations and service-related queries, synergizing advanced NLP techniques, Large Language Models, and LLM prompt engineering.

# Key Features
RAG Application

Specialized Responses

Integration & Framework

# Technologies and Methods Used

# Vector Stores
One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors. At query time, the unstructured query is embedded, and the embedding vectors that are 'most similar' to the embedded query are retrieved. A vector store takes care of storing embedded data and performing vector search. Learn more about it here.

# ConversationalRetrievalQA
This chain builds on RetrievalQAChain, providing a chat history component. It combines chat history and the question to retrieve relevant documents, which are then passed to a question-answering chain for responses. More details can be found here.

# LangChain
LangChain is a unique framework designed for applications powered by language models. It's data-aware, agentic and is primarily characterized by its components and off-the-shelf chains. Learn more about its capabilities here.

# FAISS
Facebook AI Similarity Search (FAISS) is a library crafted to facilitate developers in the swift search of multimedia document embeddings that resemble each other, solving traditional query search engines' limitations.

# Streamlit
The user interface of Wendy AI is powered by Streamlit, offering an interactive and user-friendly experience. The project is hosted on the Streamlit Community cloud.

# Retrieval Augmented Generation (RAG)
RAG is an AI framework that retrieves data from external sources to ground Large Language Models (LLMs) on accurate, recent information, thus enhancing the LLMs' generative process. The principle of RAG ensures that the LLM responses are grounded on the most recent and reliable sources, supplementing the LLMâ€™s internal information representation. Further reading on RAG.

# Contributing ðŸ™Œ
If you want to contribute to this project, please open an issue, submit a pull request or contact me at pvantipalli99@gmail.com
